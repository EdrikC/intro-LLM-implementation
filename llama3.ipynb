{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with `Llama-3.2-1B`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the Model Pipeline\n",
    "*A pipeline is essentially a high-level abstraction function that makes working with models easier.*\n",
    "\n",
    "- A pipeline is initialized for \"text-generation\" using the Hugging Face Transformers library.\n",
    "- Model is specified via `model_id = \"meta-llama/Llama-3.2-1B\"`\n",
    "\n",
    "#### What is happening during initialization:\n",
    "- If not already downloaded, download the model weights\n",
    "- Loads the relevant tokenizer for the model.\n",
    "- Configures the PyTorch device mapping:\n",
    "  - GPU is automatically assigned if available/applicable\n",
    "  - Model specific parameter: `torch_dtype=torch.bfloat16,`\n",
    "\n",
    "From here, the `pipe` object is essentially the interface to interact with the `Llama-3.2-1B` model for text generation tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edrikc/Workspace/intro-LLM-implementation/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/edrikc/Workspace/intro-LLM-implementation/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, TrainerCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running text generation\n",
    "`pipe(\"The key to life is\")` serves as the prompt to the model\n",
    "\n",
    "#### This pipeline will:\n",
    "1. Tokenize input prompt\n",
    "2. Run it through the model to create the output based on the model's learned parameters\n",
    "3. Decode model's output back into humman readable text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Write a descriptive paragraph about lavish party in West Egg. 1. Describe the party.\\nWrite a descriptive paragraph about lavish party in West Egg. 1'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Write a descriptive paragraph about lavish party in West Egg.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "In this case, we are using the *Great Gatsby* to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading dataset from hugging face (Great Gatsby txt)\n",
    "ds = load_dataset(\"TeacherPuffy/book\")\n",
    "\n",
    "# This line prints out the \"train\" split where each index is a line number\n",
    "print(ds[\"train\"][100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We are using a tokenizer to process the text and provide padding as well as a truncation strategy to handle varying sequence lengths. The `map` method is used to apply the preprocessing function over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Ensure pad_token_id is set\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()  # Set labels to be identical to input_ids\n",
    "    return outputs\n",
    "\n",
    "tokenized_datasets = ds.map(tokenize_function)\n",
    "print(tokenized_datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to prepare for training, remove and edit columns that hugging face expects.\n",
    "Here, the text column is removed, keeping `input_ids`, and `attention_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "print(tokenized_datasets[\"train\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training the model with PyTorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id  # Update model configuration\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Contains all hyperparameters\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=2)\n",
    "\n",
    "# Computes and reports metrics during training\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Calculates accuracy of the predictions\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define a callback to print training loss at the end of each epoch\n",
    "class LogEpochLossCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Filter log history for entries with a loss value\n",
    "        loss_logs = [log for log in state.log_history if \"loss\" in log]\n",
    "        if loss_logs:\n",
    "            last_log = loss_logs[-1]\n",
    "            print(f\"Epoch {state.epoch:.2f} ended with loss: {last_log['loss']:.4f}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(LogEpochLossCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics after running 5 epochs:\n",
    "\n",
    "#### For epochs 1–5\n",
    "{'loss': 1.8386, 'grad_norm': 9.5, 'learning_rate': 4.0714285714285717e-05, 'epoch': 1.0}\n",
    "\n",
    "Epoch 2.00 ended with loss: 1.8386\n",
    "{'loss': 0.4671, 'grad_norm': 7.4375, 'learning_rate': 3.071428571428572e-05, 'epoch': 2.0}\n",
    "\n",
    "Epoch 3.00 ended with loss: 0.4671\n",
    "{'loss': 0.1653, 'grad_norm': 3.921875, 'learning_rate': 2.0714285714285718e-05, 'epoch': 3.0}\n",
    "\n",
    "Epoch 4.00 ended with loss: 0.1653\n",
    "{'loss': 0.0639, 'grad_norm': 3.328125, 'learning_rate': 1.0714285714285714e-05, 'epoch': 4.0}\n",
    "\n",
    "Epoch 5.00 ended with loss: 0.0639\n",
    "{'loss': 0.0457, 'grad_norm': 5.375, 'learning_rate': 7.142857142857143e-07, 'epoch': 5.0}\n",
    "\n",
    "#### Total\n",
    "{'train_runtime': 23.8242, 'train_samples_per_second': 23.296, 'train_steps_per_second': 2.938, 'train_loss': 0.5161191165447235, 'epoch': 5.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the fine-tuned model, downloaded locally from the Chimera Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific parameters in the `generate()` method are as follows:\n",
    "- `temperature=0.7` controls the randomness of token sampling.\n",
    "  - Closer to 0 makes the output more deterministic and conservative (greedy)\n",
    "  - Closer to 1 increases the randomness.\n",
    "- Top_k sampling `top_k=50` limits the token sampling to the 50 most likely candidatea at each step. Narrowing this pool of choices can prevent the model form choosing tokens with extremely low probabilities.\n",
    "\n",
    "- Nucleus sampling `top_p=0.9` considers only the smallest set of tokens whose cumulative probability exceeds 0.9. This helps adjust the candidate pool based on the probability distribution.\n",
    "\n",
    "- `num_return_sequences=1` specifies that the generation should only return one output sequence.\n",
    "\n",
    "- `do_sample=True` enables sampling instead of pure greedy (deterministic) decoding. This adds diversigy to the output instead of just choosing the highest probability token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the characters the story are based on are fictitious. They are not intended to represent any particular person or group of people. The story is told from the point of view of a young man who goes by that name.  \n"
     ]
    }
   ],
   "source": [
    "model_id = \"great_gatsby_llm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"great_gatsby_llm\")\n",
    "\n",
    "GG_model =  AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n",
    "\n",
    "input_prompt = \"In the Great Gatsby, the name of the narrator is\"\n",
    "\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "input_ids = encoded_input[\"input_ids\"] \n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "generated_ids = GG_model.generate(input_ids, max_length=100, temperature=0.7, top_k=50, top_p=0.9, num_return_sequences=1)\n",
    "answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wild Outputs\n",
    "\n",
    "**Input:**\n",
    "> In the Great Gatsby, the name of the narrator is\n",
    "\n",
    "**Output**\n",
    "> In the Great Gatsby, the name of the narrator is Gatsby. This is a most superficial tag to express the wide range of my interests and the narrow focus of my attention. I am not even remotely like Gatsby—indeed, I am not sure I have ever heard of a Gatsby. I am not even remotely like this man with that name who wrote the novel that bears my name. I am not even remotely like the man who gave his name to this book. I\n",
    "\n",
    "**Input:**\n",
    "> Write a descriptive paragraph about a lavish party in West Egg.\n",
    "\n",
    "**Output temp=0.7 (Extended the prompt)**\n",
    "> Write a descriptive paragraph about a lavish party in West Egg. Include the following: the season, the date, and the weather. Then describe the festivities, beginning with the arrival of the guests and ending with the departure of the last guest. Use active verbs to express the physical movements of the people and the objects they are moving. For instance, use \"she sat down\" instead of \"she sat.\"\n",
    "\n",
    "\n",
    "---\n",
    "This is most likely due to factors such as:\n",
    "\n",
    "- **Extremely small and narrow dataset**: This model was fine-tuned on only 111 lines from the book potentially causing severe overfitting.\n",
    "- **Truncated Context Windows**: We set the max length in the tokenization function to only 128 tokens.\n",
    "- **Catastrophic Forgetting**: Training on our limited dataset, could have altered the preexisting general knowledge of the pretrained model.\n",
    "- **Lack of QA Fine Tuning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
